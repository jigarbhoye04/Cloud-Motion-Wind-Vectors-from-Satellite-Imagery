{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install yacs loguru einops timm==0.4.12 imageio gdown\n!pip install opencv-python-headless\nimport subprocess\nimport sys\n\n# Install additional requirements if needed\ntry:\n    import yacs\n    import loguru  \n    import einops\n    print(\"✅ All packages installed successfully!\")\nexcept ImportError as e:\n    print(f\"❌ Missing package: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Clone Repository and Setup","metadata":{}},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working')\n\n# Clone the FlowFormer++ repository\n!git clone https://github.com/XiaoyuShi97/FlowFormerPlusPlus.git\nos.chdir('/kaggle/working/FlowFormerPlusPlus')\n\n# Also clone the original FlowFormer for compatibility\n!git clone https://github.com/drinkingcoder/FlowFormer-Official.git FlowFormer-Original\n\nprint(\"✅ Repository cloned successfully!\")\nprint(f\"Current directory: {os.getcwd()}\")\nprint(f\"Contents: {os.listdir('.')}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Download Pre-trained Models (Multiple Methods)","metadata":{}},{"cell_type":"code","source":"import gdown\nimport os\nimport urllib.request\nimport requests\nfrom pathlib import Path\n\n# Create checkpoints directory\nos.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n\ndef download_with_requests(url, filepath):\n    \"\"\"Download using requests with headers\"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n    response = requests.get(url, headers=headers, stream=True)\n    if response.status_code == 200:\n        with open(filepath, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n        return True\n    return False\n\ndef download_model(model_name, file_id):\n    \"\"\"Try multiple download methods\"\"\"\n    output_path = f'/kaggle/working/checkpoints/{model_name}'\n    \n    if os.path.exists(output_path):\n        print(f\"✅ {model_name} already exists\")\n        return True\n    \n    print(f\"⬇️ Downloading {model_name}...\")\n    \n    # Method 1: Try gdown with fuzzy matching\n    try:\n        url = f'https://drive.google.com/uc?id={file_id}'\n        gdown.download(url, output_path, quiet=False, fuzzy=True)\n        if os.path.exists(output_path) and os.path.getsize(output_path) > 1000:\n            print(f\"✅ Downloaded {model_name} with gdown\")\n            return True\n    except Exception as e:\n        print(f\"⚠️ gdown failed for {model_name}: {e}\")\n    \n    # Method 2: Try direct download\n    try:\n        direct_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n        if download_with_requests(direct_url, output_path):\n            if os.path.exists(output_path) and os.path.getsize(output_path) > 1000:\n                print(f\"✅ Downloaded {model_name} with requests\")\n                return True\n    except Exception as e:\n        print(f\"⚠️ requests failed for {model_name}: {e}\")\n    \n    # Method 3: Try wget\n    try:\n        wget_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n        os.system(f'wget --no-check-certificate \"{wget_url}\" -O \"{output_path}\"')\n        if os.path.exists(output_path) and os.path.getsize(output_path) > 1000:\n            print(f\"✅ Downloaded {model_name} with wget\")\n            return True\n    except Exception as e:\n        print(f\"⚠️ wget failed for {model_name}: {e}\")\n    \n    print(f\"❌ All download methods failed for {model_name}\")\n    return False\n\n# FlowFormer++ pre-trained weights with updated IDs\nmodel_urls = {\n    'chairs.pth': '1_wJoB4lxrgAhxZOlBe7kE8KbvLXkjMdM',\n    'things.pth': '1FqKjAf8XCMHDb5wXELgqWg6I9T30GYQK', \n    'sintel.pth': '1CvK3YmvdJgGHzGaOWXc6sW6_7JT4cXrt',\n    'kitti.pth': '1TT5649zKQ7B8FgVEsF7UrPHT4TczFY6W',\n    'flowformer-small.pth': '1gao8HQvNJaHU_Rp5QqkN9SwCGS8Hf3J8',\n    'things_kitti.pth': '1Nx4RFpvgcvNgcSxaUh8wXvxqcWngtEhG'\n}\n\nprint(\"Downloading pre-trained models...\")\nsuccessful_downloads = 0\n\nfor model_name, file_id in model_urls.items():\n    if download_model(model_name, file_id):\n        successful_downloads += 1\n\nprint(f\"\\n🎯 Successfully downloaded {successful_downloads}/{len(model_urls)} models\")\n\n# Alternative: Download from alternative sources\nif successful_downloads == 0:\n    print(\"\\n🔄 Trying alternative download sources...\")\n    \n    # Try HuggingFace hub if available\n    try:\n        !pip install huggingface-hub -q\n        from huggingface_hub import hf_hub_download\n        \n        # Check if FlowFormer models are available on HuggingFace\n        # This is just an example - you'd need to find the actual model repos\n        print(\"⚠️ No HuggingFace FlowFormer models found\")\n        print(\"💡 Consider uploading models as Kaggle dataset\")\n        \n    except Exception as e:\n        print(f\"⚠️ HuggingFace hub not available: {e}\")\n    \n    # Manual download instructions\n    print(\"\\n📋 Manual Download Instructions:\")\n    print(\"1. Visit: https://drive.google.com/drive/folders/1K2dcWxaqOLiQ3PoqRdokrgWsGIf3yBA_\")\n    print(\"2. Download models manually and upload as Kaggle dataset\")\n    print(\"3. Or use the working demo mode without pre-trained models\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell: Debug Kaggle Directory Structure and Fix Datasets\n\nimport os\nimport glob\nfrom pathlib import Path\n\nprint(\"🔍 KAGGLE DIRECTORY STRUCTURE ANALYSIS\")\nprint(\"=\" * 50)\n\n# 1. Check current working directory\nprint(f\"📍 Current working directory: {os.getcwd()}\")\n\n# 2. Check Kaggle input directory structure\nprint(\"\\n📁 KAGGLE INPUT DIRECTORY STRUCTURE:\")\nkaggle_input = \"/kaggle/input\"\nif os.path.exists(kaggle_input):\n    try:\n        for root, dirs, files in os.walk(kaggle_input):\n            level = root.replace(kaggle_input, '').count(os.sep)\n            indent = ' ' * 2 * level\n            print(f\"{indent}📂 {os.path.basename(root)}/\")\n            subindent = ' ' * 2 * (level + 1)\n            for file in files[:5]:  # Show first 5 files per directory\n                size = os.path.getsize(os.path.join(root, file)) / (1024*1024)\n                print(f\"{subindent}📄 {file} ({size:.1f}MB)\")\n            if len(files) > 5:\n                print(f\"{subindent}... and {len(files)-5} more files\")\n            if level > 2:  # Limit depth to avoid too much output\n                break\n    except Exception as e:\n        print(f\"❌ Error reading kaggle input: {e}\")\nelse:\n    print(\"❌ /kaggle/input not found\")\n\n# 3. Check working directory structure  \nprint(f\"\\n📁 WORKING DIRECTORY STRUCTURE:\")\nworking_dir = \"/kaggle/working\"\nif os.path.exists(working_dir):\n    for root, dirs, files in os.walk(working_dir):\n        level = root.replace(working_dir, '').count(os.sep)\n        if level < 3:  # Limit depth\n            indent = ' ' * 2 * level\n            print(f\"{indent}📂 {os.path.basename(root)}/\")\n            subindent = ' ' * 2 * (level + 1)\n            for file in files[:3]:  # Show first 3 files per directory\n                print(f\"{subindent}📄 {file}\")\n            if len(files) > 3:\n                print(f\"{subindent}... and {len(files)-3} more files\")\n\n# 4. Search for any optical flow related files\nprint(f\"\\n🔍 SEARCHING FOR OPTICAL FLOW DATASETS:\")\nsearch_patterns = [\n    \"/kaggle/input/**/Sintel**\",\n    \"/kaggle/input/**/KITTI**\", \n    \"/kaggle/input/**/FlyingChairs**\",\n    \"/kaggle/input/**/FlyingThings**\",\n    \"/kaggle/input/**/*sintel*\",\n    \"/kaggle/input/**/*kitti*\",\n    \"/kaggle/input/**/*flying*\",\n    \"/kaggle/input/**/*optical*\",\n    \"/kaggle/input/**/*flow*\"\n]\n\nfound_datasets = []\nfor pattern in search_patterns:\n    matches = glob.glob(pattern, recursive=True)\n    for match in matches:\n        if os.path.isdir(match):\n            found_datasets.append(match)\n            print(f\"📂 Found: {match}\")\n\n# 5. List all available kaggle input datasets\nprint(f\"\\n📊 ALL AVAILABLE KAGGLE DATASETS:\")\nif os.path.exists(\"/kaggle/input\"):\n    datasets = [d for d in os.listdir(\"/kaggle/input\") if os.path.isdir(f\"/kaggle/input/{d}\")]\n    for i, dataset in enumerate(datasets, 1):\n        dataset_path = f\"/kaggle/input/{dataset}\"\n        try:\n            # Count files in dataset\n            file_count = sum([len(files) for r, d, files in os.walk(dataset_path)])\n            # Get dataset size\n            total_size = sum([os.path.getsize(os.path.join(root, file)) \n                            for root, dirs, files in os.walk(dataset_path) \n                            for file in files]) / (1024*1024*1024)  # GB\n            print(f\"  {i:2d}. {dataset} ({file_count} files, {total_size:.2f}GB)\")\n        except:\n            print(f\"  {i:2d}. {dataset} (access error)\")\n\n# 6. Check if datasets contain optical flow data\nprint(f\"\\n🔍 CHECKING DATASETS FOR OPTICAL FLOW CONTENT:\")\nif os.path.exists(\"/kaggle/input\"):\n    for dataset in os.listdir(\"/kaggle/input\"):\n        dataset_path = f\"/kaggle/input/{dataset}\"\n        if os.path.isdir(dataset_path):\n            # Look for common optical flow file patterns\n            flow_indicators = []\n            try:\n                for root, dirs, files in os.walk(dataset_path):\n                    for file in files[:10]:  # Check first 10 files\n                        if any(keyword in file.lower() for keyword in ['flow', 'sintel', 'kitti', 'flying']):\n                            flow_indicators.append(file)\n                    for dir_name in dirs:\n                        if any(keyword in dir_name.lower() for keyword in ['flow', 'sintel', 'kitti', 'flying', 'optical']):\n                            flow_indicators.append(f\"📂{dir_name}\")\n                    if len(flow_indicators) >= 5:\n                        break\n                        \n                if flow_indicators:\n                    print(f\"🎯 {dataset}: {flow_indicators[:5]}\")\n                    if len(flow_indicators) > 5:\n                        print(f\"    ... and {len(flow_indicators)-5} more matches\")\n                        \n            except Exception as e:\n                print(f\"❌ Error checking {dataset}: {e}\")\n\nprint(f\"\\n\" + \"=\" * 50)\nprint(\"🎯 SOLUTION RECOMMENDATIONS:\")\nprint(\"1. Add these datasets to your notebook:\")\nprint(\"   - Search 'Sintel optical flow' on Kaggle\")  \nprint(\"   - Search 'KITTI optical flow' on Kaggle\")\nprint(\"   - Search 'FlyingChairs dataset' on Kaggle\")\nprint(\"   - Search 'FlyingThings3D' on Kaggle\")\nprint(\"2. Or create synthetic demo data (see next cell)\")\nprint(\"3. Or modify dataset paths based on what you found above\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell: Perfect Dataset Linking for Your Specific Setup\n\nimport os\nimport shutil\n\nprint(\"🎯 LINKING YOUR SPECIFIC DATASETS\")\nprint(\"=\" * 40)\n\n# Create datasets directory\ndataset_base = '/kaggle/working/FlowFormerPlusPlus/datasets'\nos.makedirs(dataset_base, exist_ok=True)\n\ndef create_link(source, target_name):\n    \"\"\"Create symbolic link with fallback to copy\"\"\"\n    target_path = f\"{dataset_base}/{target_name}\"\n    \n    if os.path.exists(target_path):\n        print(f\"✅ {target_name} already exists\")\n        return True\n        \n    try:\n        os.symlink(source, target_path)\n        print(f\"✅ Linked {source} → {target_name}\")\n        return True\n    except Exception as e:\n        print(f\"⚠️ Symlink failed for {target_name}: {e}\")\n        return False\n\n# Link your specific datasets\nsuccess_count = 0\n\n# 1. Link MPI-Sintel Dataset\nif create_link('/kaggle/input/mpi-sintel-dataset', 'Sintel'):\n    success_count += 1\n\n# 2. Link KITTI Dataset\nif create_link('/kaggle/input/kitti-unified-optical-flow', 'KITTI'):\n    success_count += 1\n\n# 3. Check the internal structure and create proper KITTI links\nprint(f\"\\n🔍 Setting up KITTI internal structure...\")\nkitti_base = '/kaggle/input/kitti-unified-optical-flow'\nkitti_target = f'{dataset_base}/KITTI'\n\nif os.path.exists(kitti_base):\n    # The KITTI dataset has kitti_2012_2015_virtual2 structure\n    kitti_2015_path = f'{kitti_base}/kitti_2012_2015_virtual2/kitti_2015'\n    kitti_2012_path = f'{kitti_base}/kitti_2012_2015_virtual2/kitti_2012'\n    \n    if os.path.exists(kitti_2015_path):\n        # Create additional direct links for easier access\n        os.makedirs(f'{dataset_base}/KITTI_2015', exist_ok=True)\n        try:\n            if not os.path.exists(f'{dataset_base}/KITTI_2015/training'):\n                os.symlink(f'{kitti_2015_path}/training', f'{dataset_base}/KITTI_2015/training')\n            if not os.path.exists(f'{dataset_base}/KITTI_2015/testing') and os.path.exists(f'{kitti_2015_path}/testing'):\n                os.symlink(f'{kitti_2015_path}/testing', f'{dataset_base}/KITTI_2015/testing')\n            print(f\"✅ Linked KITTI 2015 internal structure\")\n        except Exception as e:\n            print(f\"⚠️ KITTI 2015 internal linking failed: {e}\")\n\n# 4. Verify dataset structures\nprint(f\"\\n🔍 VERIFYING DATASET STRUCTURES...\")\n\ndef verify_dataset(dataset_name, required_paths):\n    \"\"\"Verify that dataset has the expected structure\"\"\"\n    base_path = f\"{dataset_base}/{dataset_name}\"\n    if not os.path.exists(base_path):\n        print(f\"❌ {dataset_name}: Base path not found\")\n        return False\n    \n    found_paths = []\n    missing_paths = []\n    \n    for req_path in required_paths:\n        full_path = f\"{base_path}/{req_path}\"\n        if os.path.exists(full_path):\n            found_paths.append(req_path)\n        else:\n            missing_paths.append(req_path)\n    \n    if found_paths:\n        print(f\"✅ {dataset_name}: Found {found_paths}\")\n    if missing_paths:\n        print(f\"⚠️ {dataset_name}: Missing {missing_paths}\")\n    \n    return len(found_paths) > 0\n\n# Verify Sintel structure\nsintel_paths = ['training/clean', 'training/final', 'training/flow', 'test']\nverify_dataset('Sintel', sintel_paths)\n\n# Verify KITTI structure  \nkitti_paths = ['training', 'testing', 'kitti_2012_2015_virtual2']\nverify_dataset('KITTI', kitti_paths)\n\n# 5. List final dataset structure\nprint(f\"\\n📁 FINAL DATASET STRUCTURE:\")\nprint(f\"Base directory: {dataset_base}\")\n\nfor item in os.listdir(dataset_base):\n    item_path = os.path.join(dataset_base, item)\n    if os.path.isdir(item_path):\n        print(f\"\\n📂 {item}/\")\n        try:\n            # Show first 2 levels of subdirectories\n            for root, dirs, files in os.walk(item_path):\n                level = root.replace(item_path, '').count(os.sep)\n                if level < 2:\n                    indent = '  ' * (level + 1)\n                    for dir_name in dirs[:5]:  # Show first 5 dirs\n                        subdir_path = os.path.join(root, dir_name)\n                        if os.path.isdir(subdir_path):\n                            file_count = len(os.listdir(subdir_path)) if os.access(subdir_path, os.R_OK) else 0\n                            print(f\"{indent}📂 {dir_name}/ ({file_count} items)\")\n                    if len(dirs) > 5:\n                        print(f\"{indent}... and {len(dirs)-5} more directories\")\n                    \n                    # Show a few files\n                    for file_name in files[:3]:\n                        print(f\"{indent}📄 {file_name}\")\n                    if len(files) > 3:\n                        print(f\"{indent}... and {len(files)-3} more files\")\n        except Exception as e:\n            print(f\"  ⚠️ Error reading structure: {e}\")\n\nprint(f\"\\n🎯 DATASET SETUP COMPLETE!\")\nprint(f\"✅ Successfully linked: {success_count}/2 main datasets\")\nprint(f\"💡 You have excellent datasets for FlowFormer++ evaluation!\")\n\n# Set environment variables for next cells\nos.environ['FLOWFORMER_HAS_SINTEL'] = 'true'\nos.environ['FLOWFORMER_HAS_KITTI'] = 'true'\nos.environ['FLOWFORMER_DATASETS_READY'] = 'true'\n\nprint(f\"\\n🚀 Ready to proceed with FlowFormer++ evaluation!\")\nprint(f\"   → Continue with Cell 5 (Verify Installation)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell: Clean Up Unnecessary Files and Directories\n\nimport os\nimport shutil\nimport glob\n\nprint(\"🧹 CLEANING UP UNNECESSARY FILES\")\nprint(\"=\" * 40)\n\ndef safe_remove(path, item_type=\"file\"):\n    \"\"\"Safely remove files or directories with confirmation\"\"\"\n    try:\n        if os.path.exists(path):\n            if item_type == \"file\":\n                os.remove(path)\n                print(f\"🗑️ Removed file: {path}\")\n            elif item_type == \"dir\":\n                shutil.rmtree(path)\n                print(f\"🗑️ Removed directory: {path}\")\n            return True\n        else:\n            print(f\"⚠️ Not found: {path}\")\n            return False\n    except Exception as e:\n        print(f\"❌ Failed to remove {path}: {e}\")\n        return False\n\ndef get_dir_size(path):\n    \"\"\"Get directory size in MB\"\"\"\n    try:\n        total_size = 0\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                if os.path.exists(fp):\n                    total_size += os.path.getsize(fp)\n        return total_size / (1024 * 1024)  # Convert to MB\n    except:\n        return 0\n\n# 1. Remove macOS system files (.AppleDouble, .DS_Store)\nprint(\"🍎 Removing macOS system files...\")\napple_files = []\ndataset_base = '/kaggle/working/FlowFormerPlusPlus/datasets'\n\nfor root, dirs, files in os.walk(dataset_base):\n    # Remove .AppleDouble directories\n    for d in dirs:\n        if d == '.AppleDouble':\n            apple_path = os.path.join(root, d)\n            apple_files.append(apple_path)\n    \n    # Remove .DS_Store files\n    for f in files:\n        if f == '.DS_Store':\n            ds_path = os.path.join(root, f)\n            apple_files.append(ds_path)\n\nremoved_count = 0\nfor apple_file in apple_files:\n    if safe_remove(apple_file, \"dir\" if \".AppleDouble\" in apple_file else \"file\"):\n        removed_count += 1\n\nprint(f\"✅ Removed {removed_count} macOS system files/directories\")\n\n# 2. Clean up unnecessary KITTI night datasets (keep only main datasets)\nprint(f\"\\n🌙 Cleaning up unnecessary KITTI night datasets...\")\nkitti_base = f\"{dataset_base}/KITTI\"\n\nunnecessary_kitti = [\n    f\"{kitti_base}/kitti_2015_image_2_night\",\n    f\"{kitti_base}/kitti_2012_colored_0_night\"\n]\n\nkitti_cleaned = 0\nfor night_dir in unnecessary_kitti:\n    if os.path.exists(night_dir):\n        size_mb = get_dir_size(night_dir)\n        print(f\"📊 {os.path.basename(night_dir)}: {size_mb:.1f}MB\")\n        if safe_remove(night_dir, \"dir\"):\n            kitti_cleaned += 1\n            print(f\"💾 Freed {size_mb:.1f}MB\")\n\nif kitti_cleaned == 0:\n    print(\"✅ No KITTI night datasets found (already clean)\")\n\n# 3. Clean up duplicate FlowFormer repository\nprint(f\"\\n📂 Removing duplicate FlowFormer repository...\")\nduplicate_repo = '/kaggle/working/FlowFormerPlusPlus/FlowFormer-Original'\nif os.path.exists(duplicate_repo):\n    size_mb = get_dir_size(duplicate_repo)\n    print(f\"📊 FlowFormer-Original size: {size_mb:.1f}MB\")\n    if safe_remove(duplicate_repo, \"dir\"):\n        print(f\"💾 Freed {size_mb:.1f}MB\")\nelse:\n    print(\"✅ No duplicate repository found\")\n\n# 4. Remove unnecessary bundler and flow_code directories from Sintel\nprint(f\"\\n🎬 Cleaning up unnecessary Sintel components...\")\nsintel_base = f\"{dataset_base}/Sintel\"\nunnecessary_sintel = [\n    f\"{sintel_base}/bundler\",\n    f\"{sintel_base}/flow_code\"\n]\n\nsintel_cleaned = 0\nfor sintel_dir in unnecessary_sintel:\n    if os.path.exists(sintel_dir):\n        size_mb = get_dir_size(sintel_dir)\n        print(f\"📊 {os.path.basename(sintel_dir)}: {size_mb:.1f}MB\")\n        # Only remove if it's not essential (flow_code might be useful)\n        if \"bundler\" in sintel_dir:  # Remove bundler, keep flow_code\n            if safe_remove(sintel_dir, \"dir\"):\n                sintel_cleaned += 1\n                print(f\"💾 Freed {size_mb:.1f}MB\")\n        else:\n            print(f\"⚠️ Keeping {os.path.basename(sintel_dir)} (might be useful)\")\n\n# 5. Clean up Git directory (large and unnecessary)\nprint(f\"\\n🔧 Cleaning up Git repository data...\")\ngit_dir = '/kaggle/working/FlowFormerPlusPlus/.git'\nif os.path.exists(git_dir):\n    size_mb = get_dir_size(git_dir)\n    print(f\"📊 .git directory: {size_mb:.1f}MB\")\n    if safe_remove(git_dir, \"dir\"):\n        print(f\"💾 Freed {size_mb:.1f}MB\")\nelse:\n    print(\"✅ No .git directory found\")\n\n# 6. Clean up unnecessary virtual documents\nprint(f\"\\n📝 Cleaning up virtual documents...\")\nvirtual_docs = '/kaggle/working/.virtual_documents'\nif os.path.exists(virtual_docs):\n    size_mb = get_dir_size(virtual_docs)\n    print(f\"📊 .virtual_documents: {size_mb:.1f}MB\")\n    if safe_remove(virtual_docs, \"dir\"):\n        print(f\"💾 Freed {size_mb:.1f}MB\")\nelse:\n    print(\"✅ No virtual documents found\")\n\n# 7. Clean up any Python cache files\nprint(f\"\\n🐍 Cleaning up Python cache files...\")\ncache_patterns = [\n    '/kaggle/working/**/__pycache__',\n    '/kaggle/working/**/*.pyc',\n    '/kaggle/working/**/*.pyo'\n]\n\ncache_removed = 0\nfor pattern in cache_patterns:\n    for cache_item in glob.glob(pattern, recursive=True):\n        if os.path.isdir(cache_item):\n            if safe_remove(cache_item, \"dir\"):\n                cache_removed += 1\n        elif os.path.isfile(cache_item):\n            if safe_remove(cache_item, \"file\"):\n                cache_removed += 1\n\nprint(f\"✅ Removed {cache_removed} Python cache files/directories\")\n\n# 8. Summary of cleaned directory structure\nprint(f\"\\n📁 CLEANED DIRECTORY STRUCTURE:\")\nprint(f\"Base directory: {dataset_base}\")\n\nfor item in sorted(os.listdir(dataset_base)):\n    item_path = os.path.join(dataset_base, item)\n    if os.path.isdir(item_path):\n        size_mb = get_dir_size(item_path)\n        print(f\"\\n📂 {item}/ ({size_mb:.1f}MB)\")\n        \n        # Show clean structure (first level only)\n        try:\n            subdirs = [d for d in os.listdir(item_path) if os.path.isdir(os.path.join(item_path, d))]\n            files = [f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))]\n            \n            for subdir in sorted(subdirs[:5]):  # Show first 5 subdirs\n                subdir_path = os.path.join(item_path, subdir)\n                subdir_items = len(os.listdir(subdir_path)) if os.access(subdir_path, os.R_OK) else 0\n                print(f\"  📂 {subdir}/ ({subdir_items} items)\")\n            if len(subdirs) > 5:\n                print(f\"  📂 ... and {len(subdirs)-5} more directories\")\n                \n            for file in sorted(files[:3]):  # Show first 3 files\n                print(f\"  📄 {file}\")\n            if len(files) > 3:\n                print(f\"  📄 ... and {len(files)-3} more files\")\n                \n        except Exception as e:\n            print(f\"  ⚠️ Error reading: {e}\")\n\nprint(f\"\\n🎯 CLEANUP COMPLETE!\")\nprint(f\"✅ Directory structure is now clean and optimized\")\nprint(f\"💾 Freed significant disk space\")\nprint(f\"🚀 Ready for efficient FlowFormer++ evaluation!\")\n\n# Verify essential directories still exist\nessential_dirs = [\n    f\"{dataset_base}/Sintel/training/clean\",\n    f\"{dataset_base}/Sintel/training/final\", \n    f\"{dataset_base}/Sintel/training/flow\",\n    f\"{dataset_base}/KITTI/kitti_2012_2015_virtual2/kitti_2015\"\n]\n\nprint(f\"\\n✅ VERIFICATION - Essential directories:\")\nfor essential in essential_dirs:\n    exists = os.path.exists(essential)\n    status = \"✅\" if exists else \"❌\"\n    print(f\"  {status} {essential.replace(dataset_base + '/', '')}\")\n\nprint(f\"\\n💡 Next: Continue with Cell 5 (Verify Installation)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setup Dataset Paths","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Create expected directory structure\ndataset_base = '/kaggle/working/FlowFormerPlusPlus/datasets'\nos.makedirs(dataset_base, exist_ok=True)\n\n# Link Kaggle input datasets to expected paths\nkaggle_input = '/kaggle/input'\n\n# Function to create symbolic links or copy data\ndef setup_dataset_link(kaggle_path, target_path):\n    if os.path.exists(kaggle_path):\n        if not os.path.exists(target_path):\n            try:\n                os.symlink(kaggle_path, target_path)\n                print(f\"✅ Linked {kaggle_path} → {target_path}\")\n            except:\n                # If symlink fails, try copying (for small datasets)\n                if os.path.getsize(kaggle_path) < 1e9:  # Less than 1GB\n                    shutil.copytree(kaggle_path, target_path)\n                    print(f\"✅ Copied {kaggle_path} → {target_path}\")\n                else:\n                    print(f\"⚠️ Dataset too large to copy: {kaggle_path}\")\n        else:\n            print(f\"✅ {target_path} already exists\")\n    else:\n        print(f\"❌ Dataset not found: {kaggle_path}\")\n\n# Setup dataset links (adjust paths based on your added datasets)\ndataset_mappings = {\n    'sintel-dataset': 'Sintel',\n    'kitti-optical-flow': 'KITTI', \n    'flyingchairs-release': 'FlyingChairs_release',\n    'flyingthings3d-optical-flow': 'FlyingThings3D'\n}\n\nprint(\"Setting up dataset links...\")\nfor kaggle_name, folder_name in dataset_mappings.items():\n    kaggle_path = f'/kaggle/input/{kaggle_name}'\n    target_path = f'{dataset_base}/{folder_name}'\n    setup_dataset_link(kaggle_path, target_path)\n\n# List available datasets\nprint(f\"\\n📁 Available datasets in {dataset_base}:\")\nif os.path.exists(dataset_base):\n    for item in os.listdir(dataset_base):\n        path = os.path.join(dataset_base, item)\n        if os.path.isdir(path):\n            print(f\"  📂 {item}\")\n        else:\n            print(f\"  📄 {item}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Verify Installation and Setup","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/FlowFormerPlusPlus')\n\n# Test imports\ntry:\n    import torch\n    import torchvision\n    import cv2\n    import numpy as np\n    \n    print(f\"✅ PyTorch version: {torch.__version__}\")\n    print(f\"✅ Torchvision version: {torchvision.__version__}\")\n    print(f\"✅ OpenCV version: {cv2.__version__}\")\n    print(f\"✅ CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"✅ CUDA device: {torch.cuda.get_device_name()}\")\n        print(f\"✅ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n    \nexcept ImportError as e:\n    print(f\"❌ Import error: {e}\")\n\n# Check if we can import FlowFormer modules\nos.chdir('/kaggle/working/FlowFormerPlusPlus')\ntry:\n    # This will test if the repository setup is correct\n    from configs.submission import get_cfg\n    print(\"✅ FlowFormer++ configuration loaded successfully!\")\nexcept Exception as e:\n    print(f\"❌ FlowFormer++ import error: {e}\")\n    # Try alternative path\n    try:\n        os.chdir('/kaggle/working/FlowFormerPlusPlus/FlowFormer-Original')\n        sys.path.append('/kaggle/working/FlowFormerPlusPlus/FlowFormer-Original')\n        print(\"✅ Using original FlowFormer repository\")\n    except:\n        print(\"❌ Could not setup FlowFormer paths\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Run Inference/Evaluation (Works with or without models)","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nos.chdir('/kaggle/working/FlowFormerPlusPlus')\n\n# Set environment variables\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n# Check available models and datasets\ncheckpoint_dir = '/kaggle/working/checkpoints'\navailable_models = os.listdir(checkpoint_dir) if os.path.exists(checkpoint_dir) else []\nhas_models = len([f for f in available_models if f.endswith('.pth') and os.path.getsize(f'{checkpoint_dir}/{f}') > 1000]) > 0\n\nprint(f\"🔍 Available models: {len(available_models)}\")\nprint(f\"📁 Has valid models: {has_models}\")\n\nif has_models:\n    print(\"🎯 Running with pre-trained models...\")\n    \n    # Run evaluation on Sintel (if dataset is available)\n    if os.path.exists('datasets/Sintel'):\n        print(\"🔥 Running Sintel evaluation...\")\n        try:\n            # Use tiling for memory efficiency\n            !python evaluate_FlowFormer_tile.py --eval sintel_validation\n        except Exception as e:\n            print(f\"❌ Sintel evaluation failed: {e}\")\n            # Try without tiling\n            try:\n                !python evaluate_FlowFormer.py --dataset sintel\n            except Exception as e2:\n                print(f\"❌ Alternative evaluation also failed: {e2}\")\n    else:\n        print(\"⚠️ Sintel dataset not available, skipping evaluation\")\n\n    # Run evaluation on KITTI (if dataset is available)  \n    if os.path.exists('datasets/KITTI'):\n        print(\"🔥 Running KITTI evaluation...\")\n        try:\n            !python evaluate_FlowFormer_tile.py --eval kitti_validation --model /kaggle/working/checkpoints/things_kitti.pth\n        except Exception as e:\n            print(f\"❌ KITTI evaluation failed: {e}\")\n    else:\n        print(\"⚠️ KITTI dataset not available, skipping evaluation\")\n\nelse:\n    print(\"⚠️ No pre-trained models available\")\n    print(\"🔄 Running in demo/training mode...\")\n    \n    # Check if we can at least load the model architecture\n    try:\n        import sys\n        sys.path.append('/kaggle/working/FlowFormerPlusPlus')\n        \n        # Try to import FlowFormer modules\n        print(\"🧪 Testing FlowFormer++ import...\")\n        \n        # This will vary depending on the exact repository structure\n        try:\n            from core.FlowFormer import FlowFormer\n            print(\"✅ FlowFormer model class imported successfully\")\n            \n            # Create model instance for testing\n            model = FlowFormer()\n            print(\"✅ FlowFormer model instantiated\")\n            print(f\"📊 Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n            \n        except Exception as e:\n            print(f\"⚠️ FlowFormer import failed: {e}\")\n            # Try alternative import\n            try:\n                from models import FlowFormer  # Alternative path\n                print(\"✅ FlowFormer imported from alternative path\")\n            except:\n                print(\"❌ Could not import FlowFormer model\")\n                \n    except Exception as e:\n        print(f\"❌ Model testing failed: {e}\")\n    \n    # Run basic training test if datasets are available\n    if os.path.exists('datasets/FlyingChairs_release') or os.path.exists('datasets'):\n        print(\"🏋️ Testing training pipeline...\")\n        try:\n            # Try a minimal training run\n            !python train_FlowFormer.py --name demo --stage chairs --validation chairs --epochs 1 --batch_size 1\n        except Exception as e:\n            print(f\"❌ Training test failed: {e}\")\n            print(\"💡 This is expected without proper datasets and configs\")\n    else:\n        print(\"⚠️ No training datasets available\")\n\n# Provide next steps guidance\nprint(\"\\n🎯 Next Steps:\")\nif has_models:\n    print(\"✅ You have pre-trained models - ready for evaluation!\")\n    print(\"💡 Add datasets to run full evaluation\")\nelse:\n    print(\"📋 To get pre-trained models:\")\n    print(\"  1. Try manual download from Google Drive\")\n    print(\"  2. Upload models as a Kaggle dataset\")\n    print(\"  3. Train from scratch (requires large datasets)\")\n    print(\"  4. Use the demo mode with synthetic data\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom Image Demo (Works without datasets)","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\n\n# Create demo images if no dataset is available\ndef create_demo_images():\n    \"\"\"Create simple demo images for testing\"\"\"\n    # Create two simple test images\n    img1 = np.zeros((480, 640, 3), dtype=np.uint8)\n    img2 = np.zeros((480, 640, 3), dtype=np.uint8)\n    \n    # Add a moving square\n    cv2.rectangle(img1, (100, 100), (200, 200), (255, 255, 255), -1)\n    cv2.rectangle(img2, (120, 110), (220, 210), (255, 255, 255), -1)  # Moved square\n    \n    return img1, img2\n\n# Create demo directory and images\ndemo_dir = '/kaggle/working/demo_data/sequence1'\nos.makedirs(demo_dir, exist_ok=True)\n\nimg1, img2 = create_demo_images()\ncv2.imwrite(f'{demo_dir}/frame1.png', img1)\ncv2.imwrite(f'{demo_dir}/frame2.png', img2)\n\nprint(\"✅ Demo images created\")\nprint(f\"📁 Demo directory: {demo_dir}\")\n\n# Visualize demo\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\naxes[0].set_title('Frame 1')\naxes[0].axis('off')\n\naxes[1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))  \naxes[1].set_title('Frame 2')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Run FlowFormer on demo sequence\nprint(\"🔥 Running FlowFormer on demo sequence...\")\ntry:\n    os.chdir('/kaggle/working/FlowFormerPlusPlus')\n    !python visualize_flow.py --eval_type seq --path /kaggle/working/demo_data/sequence1\nexcept Exception as e:\n    print(f\"❌ Demo failed: {e}\")\n    print(\"This might be due to missing model files or configuration issues.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results Visualization and Export","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\n# Look for generated results\nresults_dirs = [\n    '/kaggle/working/FlowFormerPlusPlus/demo_output',\n    '/kaggle/working/FlowFormerPlusPlus/output', \n    '/kaggle/working/FlowFormerPlusPlus/results',\n    '/kaggle/working/demo_data'\n]\n\nprint(\"🔍 Looking for results...\")\nfor results_dir in results_dirs:\n    if os.path.exists(results_dir):\n        print(f\"📁 Found results in: {results_dir}\")\n        files = os.listdir(results_dir)\n        for f in files[:10]:  # Show first 10 files\n            print(f\"  📄 {f}\")\n        if len(files) > 10:\n            print(f\"  ... and {len(files)-10} more files\")\n    else:\n        print(f\"❌ No results found in: {results_dir}\")\n\n# Display any flow visualization results\nflow_images = []\nfor results_dir in results_dirs:\n    if os.path.exists(results_dir):\n        flow_images.extend(glob(f\"{results_dir}/**/*.png\", recursive=True))\n        flow_images.extend(glob(f\"{results_dir}/**/*.jpg\", recursive=True))\n\nif flow_images:\n    print(f\"\\n🎨 Found {len(flow_images)} result images\")\n    \n    # Display first few results\n    fig, axes = plt.subplots(1, min(3, len(flow_images)), figsize=(15, 5))\n    if len(flow_images) == 1:\n        axes = [axes]\n    \n    for i, img_path in enumerate(flow_images[:3]):\n        try:\n            img = plt.imread(img_path)\n            axes[i].imshow(img)\n            axes[i].set_title(f'Result {i+1}')\n            axes[i].axis('off')\n        except Exception as e:\n            axes[i].text(0.5, 0.5, f'Error loading\\n{os.path.basename(img_path)}', \n                        ha='center', va='center')\n            axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"⚠️ No result images found\")\n\nprint(\"\\n✅ FlowFormer++ notebook execution completed!\")\nprint(\"📋 Summary:\")\nprint(\"  - Repository cloned and setup\")\nprint(\"  - Dependencies installed\") \nprint(\"  - Pre-trained models downloaded\")\nprint(\"  - Dataset paths configured\")\nprint(\"  - Demo inference attempted\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}